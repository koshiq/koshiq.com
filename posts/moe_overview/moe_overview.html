<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mixture-of-Experts (MoE) Overview</title>
    <link rel="stylesheet" type="text/css" href="../../style.css">
</head>
<body>
    <header>
        <h1>Mixture-of-Experts (MoE) Overview</h1>
        <span class="date">May 18, 2024</span>
        <br><br>
        <a href="../../index.html" class="about-me-link">Home</a>
        &nbsp; &nbsp;
        <a href="../../about/about.html" class="about-me-link">About Me</a>
        &nbsp; &nbsp;
        <a href="../../non-technical/non-technical.html" class="about-me-link">non-technical</a>
    </header>
    <main>
        <center><img src="images/MoE_architecture.png" alt="MoE Architecture" width="900" height="auto" style="margin-left:auto; margin-right: auto;"></center>
        <p><em>
            "[MoE] consists of a number of experts, each a simple feed-forward NN, and a trainable gating network which selects a sparse combination of the experts to process each input (Shazeer, 2017)...[the MoE selects] a potentially different combination of experts at each [token]. The different experts tend to become highly specialized based on syntax and semantics."
        </p></em>
        <p>
            Key ideas used in the MoEs are <span class="bold-extra-bold">sparsity, parallelism, and meta-learning.</span><br>
            <br>
            Here is a single MoE block and an example gating network inside of it.
        </p>
        <center><img src="images/MoE_block_and_gating_network.png" alt="MoE Block" width="1000" height="auto" style="margin-left:auto; margin-right: auto;"></center>
        <br><br>
        <h2>MoE Benefits:</h2>
        <br>
        <h3>1. Parallelism</h3>
        <ul>
            <li>Each expert learns to "specialize" in one "type" of data (e.g. Expert 1 focuses on cats while Expert 2 focuses on dogs for a MoE vision classifier).</li>
            <li>This allows for increased throughput as different types of data are concurrently worked on by multiple experts at once.</li>
            <br>
        </ul>
        <h3>2. Efficient Parameter Usage with Sparsity</h3>
        <ul>
            <li>While dense transformers utilize large portions of their total weights during training & inference, MoE transformers learn to selectively activate subsets of the model weights during training 7 inference.</li>
            <li>This allows light-weight, but powerful models to emerge.</li>
            <br>
        </ul>
        <h3>3. Meta-Learning</h3>
        <ul>
            <li>Classical conditional computation methods often used human-made heuristics for the model to work with, effectively decreasing their abilities to generalize to internet-scale datasets (e.g. most conditional computation vision models were often benchmarked with datasets only up to 600,000 images).</li>
            <li>With MoE transformers, the gating network is jointly trained along with the experts using back-prop, effectively decreasing the bias that may come with human-made heuristics.</li>
            <li>⇒ The question then becomes one of data-quality: "What is good data that provides the most information during training?"</li>
            <br>
        </ul>
        <h3>4. Energy-Efficiency</h3>
        <ul>
            <li>Sparsity of MoEs naturally saves energy usage both during training and inference, which decreases model training and upkeep costs along with reducing carbon footprints.</li>
            <li>Decreased number of activated parameters both during training and inference means decreased use of electricity due to less computation.</li>
            <li>GLaM, an early MoE LLM, used only ⅓ energy and ½ FLOPs for inference to be on par with GPT-3.</li>
            <br>
        </ul>
        <h2>Miscellaneous:</h2>
        <h3>Balancing of Expert Importance:</h3>
        <ul>
            <li>To prevent converging to a state where only a few experts solve everything, an extra loss term(Limportance) is included to make sure each expert is weighted equally in importance.</li>
            <br>
        </ul>
        <h3>Preventing Synchronization Overheads:</h3>
        <ul>
            <li>Effective parallelism comes from having balanced loads distributed to each component. If Expert 1 takes in 100 examples while Expert 2 takes only 1, there is a synchronization overhead.</li>
            <li>To prevent this, another loss term(Lload) is added, which reinforces Experts to have approximately equal number of examples.</li>
        <br>
        </ul>
        <h2>Downsides:</h2>
        <h3>Higher Risk of Training Instability:</h3>
        <ul>
            <li>Sparsity is a double-edged sword where it theoretically creates many discontinuities that result in NaNs or Infs.</li>
            <li>Early MoE implementations merely skipped weight updates if there were NaNs or Infs(GLaM; Du, 2017) and reverted training back to the previous checkpoint.</li>
            <li>This is another reason why large pre-training datasets were needed.</li>
        <br>
        </ul>
        <h3>Need Large Batch Sizes:</h3>
        <ul>
            <li> Larger batch sizes are needed to cover the sparsity that MoEs bring. <br> As a result, larger pre-training datasets are needed to train.</li>
        <br>
        </ul>
        <h3>Need Many GPUs:</h3>
        <ul>
            <li>Though they don't have to be hundreds of A100s, MoEs take advantage of high parallelism, which assumes you have access to many GPU devices to split computations across.</li>
        <br>
        </ul>
        <h2>Closing Thoughts:</h2>
        <h3>Scaling in low-resource environments</h3>
        <p>
            I don't think scaling is limited only to compute-rich places like google, meta, openai, etc. At the heart of scaling techniques is the effective usage of parallelism and GPU programming that leverages the characteristics of GPUs. And low-compute environments are places that need to pay attention to these the most.
        </p>
        <p>
            It's easy to get discouraged by the sheer number of parameters these MoE models carry, but they carry ideas that we should be asking in academia:
        </p>
        <ul>
            <li>"Are we using the GPUs that we have to the max? How do we increase GPU utilization?"</li>
            <li>"How can we train a better model with less data? What are the metrics for good data?"</li>
        </ul>
        <br>
    <h2>References</h2>
    <p>[1] Shazeer et al. 2017. "Outgrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer."</p>
    </main>
</body>
</html>