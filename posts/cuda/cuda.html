<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to CUDA Programming With GPU Puzzles</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet" />
    <link rel="stylesheet" type="text/css" href="../../style.css">
</head>
<body>
    <header>
        <h1>Introduction to CUDA Programming With GPU Puzzles</h1>
        <span class="date">August 29, 2024</span>
        <br><br>
        <a href="../../index.html" class="about-me-link">Home</a>
        &nbsp; &nbsp;
        <a href="../../about/about.html" class="about-me-link">About Me</a>
        &nbsp; &nbsp;
        <a href="../../non-technical/non-technical.html" class="about-me-link">non-technical</a>
    </header>
    <main>
        <p>This guide will be a hands-on approach to getting your feet wet with CUDA Programming through solving Sasha Rush’s GPU Puzzles in CUDA. </p>

        <p>We’ll cover the basics up to shared memory, focusing on foundational concepts of CUDA and NVIDIA GPUs. No base of GPU programming required, we’ll introduce all necessary ideas as we go.</p>

        <p>Let's get started!</p>

        <br>
        <h2>Puzzle 1: Map</h2>
        <p><mark style="background: #d8ebffb4; font-weight:800">"Implement a kernel that adds 10 to each position of vector `a` and stores vector `out`. You have 1 thread per position."</mark></p>

        <p>Let's take a step back and go over some terminology.<br><br></p>
        
        <h3><span style="font-weight: 1000;">Q. What is a "kernel"?</span></h3>
        <ul>
            <li>A "kernel" in GPU Programming simply means a function that runs on a GPU. But unlike functions that we are used to, GPU kernels run code in a data-parallel manner by launching multiple threads with the same instruction.</li>
        <br>
          </ul>

        <!-- <br> -->
        <h3><span style="font-weight: 1000;">Q. What is a "thread"?</span></h3>
        <ul>
            <li>They are the smallest unit of execution that individually carry out the instructions given by the kernel. We'll go deeper into threads when we talk about the <a href="#thread_hierarchy">CUDA thread hierarchy</a> below.</li>
        </ul>

        <br>
        <p>This leads us to how the CUDA programming model is one that runs the "same instruction on multiple threads." This called as the <mark style="background: #fff2cf; font-weight:800">Single Instruction Multiple Thread(SIMT)</mark> model.</p>

        <p>This gives us enough knowledge to tackle our first puzzle: we should tell each thread to take the an element from `a`, the input vector, add 10 to it, and place that in the same index of `out`, the output vector.</p>

        <p>Let's write that code.</p>

        <pre><code class="language-cpp">constexpr int VEC_SIZE = 100;     // vector size
constexpr int NUM_THREADS = 100;  // number of threads

// intermittent code(e.g. initializing data, allocating memory, etc)
// will be omitted for brevity

__global__ void addTenKernel(const float *a, float *out) {
  int local_x = threadIdx.x;      // assign one thread
  out[local_x] = a[local_x] + 10; // take one element from a, add 10, and assign to out
}</code></pre>

        <p>That was simple! No need for a for-loop or anything like that since the SIMT model lets us map the same instruction to every thread.</p>

        <p>Notice how threads are used as indexes. This will be a recurring theme throughout GPU Programming.</p>

        <p>The next puzzle is another practice of this.</p>

        <br>
        <h2>Puzzle 2: Zip</h2>
        <p><mark style="background: #d8ebffb4; font-weight:800">"Implement a kernel that adds together each position of `a` and `b` and stores it in `out`. You have 1 thread per position."</mark></p>

        <p>We can approach this puzzle just like the last one. Let's take one element from `a` and another from `b`, and store it in `out`. Of course, this is vector addition so each thread should share the same index while going from `a`, `b`, and `out`.</p>

        <p>Let's code it out.</p>

        <pre><code class="language-cpp">constexpr int VEC_SIZE = 100;    // vector size
constexpr int NUM_THREADS = 100; // number of threads

__global__ void q2_vecadd_kernel(const float *a, float *out) {
  int local_x = threadIdx.x;                 // assign one thread
  out[local_x] = a[local_x] + b[local_x];     // do vec add
}</code></pre>

        <p>Easy!</p>

        <p>But this raises the question..."What if we have too many threads? What will the remaining threads do?" After all, puzzles 1 and 2 assumed that <code>VEC_SIZE == NUM_THREADS</code>.</p>

        <p>We'll see how to manage this in Puzzle 3.</p>

        <br>
        <h2>Puzzle 3: Guards</h2>
        <p><mark style="background: #d8ebffb4; font-weight:800">"Implement a kernel that adds 10 to each position of `a` and stores it in `out`. <em>You have more threads than positions.</em>"</mark></p>

        <p>This is the exact same problem as puzzle 1, only now we have more threads than the size of the vector.</p>

        <p>To solve this problem, we can set some guardrails to prevent overflowing threads from doing anything.</p>

        <pre><code class="language-cpp">constexpr int VEC_SIZE = 100;
constexpr int NUM_THREADS = 500; // we have more threads than positions

__global__ void q3_solution_kernel(const float *a, float *out, int size) {
  int local_x = threadIdx.x;     // assign one thread
  if (local_x < size):             // check if curr thread is within vector size
    out[local_x] = a[local_x] + 10;
}</code></pre>

        <p>That completes our first part of the tutorial. Now we know how to use threads as indexes and set up boundary conditions to create GPU kernels.</p>

        <p>But notice that our examples only consisted of vectors until now. What if we wanted to work with matrixes?</p>

        <p>And to do this we need to learn about how threads are organized in CUDA.</p>

        <br>
        <h2 id="thread_hierarchy">CUDA Thread Hierarchy</h2>

        <p>Threads are organized into this hierarchy in CUDA: <code>thread -> block -> grid</code></p>
        <p>Threads come together to form a block and blocks come together to form a grid.</p>
        <img src="images/simple_thread_hierarchy.png" alt="Simple thread hierarchy" width="auto" height="400" style="margin-left:auto; margin-right: auto;">

        <br>
        <p>The above image is a simplified explanation. Blocks and grids can be 3-dimensional, which enables us to not only pack a LOT of threads into a grid, but also allows us to work with 2D matrixes and 3D tensors. Let's see an example visualization of blocks and grids that use all 3 dimensions below.</p>
        <br>
        <img src="images/multidim_thread_hierarchy.png" alt="Multidim thread hierarchy" width="auto" height="450" style="margin-left:auto; margin-right: auto;">
        <br>
        <!-- <p>For context, H100 can hold up to 270,000 threads(SXM5 version). </p> -->

        <br>
        <h3><span style="font-weight: 1000;">Q. But why do we even bother having a thread hierarchy in the first place? According to the SIMT model, aren't all threads executing the same instruction anyways?</span></h3>

        <p>This is a good question to ask, but it's more nuanced. The short answer is that it provides good abstraction for scheduling threads and allows us to write efficient software that is hardware-aware for NVIDIA GPUs.</p>

        <p>But a deeper and more interesting answer to this requires a short dive into GPU architecture and how parallel programming works on NVIDIA GPUs.</p>

        <br>
        <h3><span style="font-weight: 1000;">GPU Architecture Crash Course</span></h3>
        <br>
        <img src="images/gpu_arch.png" alt="High level GPU architecture" width="auto" height="500" style="margin-left:auto; margin-right: auto;">

        <ul>
            <li>GPUs consist of multiple Streaming Multiprocessors(SMs), L2 cache, and DRAM.</li>
            <li><mark style="background: #fff2cf;">tldr: Instructions are executed by the SMs and the data lives on the DRAM.</mark> The number of SMs and how large your DRAM is differs across architectures (e.g. my 2080ti has 68 SMs and 12GB DRAM).</li>
        </ul>

        <p>Let's have a closer look at SMs, the part that's responsible for running our kernel instructions.</p>

        <br>
        <img src="images/sm_schedule.png" alt="SM scheduling" width="auto" height="500" style="margin-left:auto; margin-right: auto;"> 
        <br>

        <p>Recall the CUDA thread hierarchy is <code>thread -> block -> grid</code>.
        Whenever a GPU kernel is called, a grid that holds multiple blocks is launched. These blocks are then distributed amongst the SMs where each SM can carry multiple blocks. By the nature of parallel programming, we can't choose which blocks will be executed first. This means that you shouldn't write any kernels that explicity expects some blocks to finish before others.</p>

        <br>
        <p>With this context, having a CUDA thread hierarchy has multiple benefits:</p>

        <ol>
                <h3>1. Abstraction</h3>
                <ul>
                    <li>Managing millions of launched threads without abstraction will be a nightmare for us. Thread hierarchy helps us understand and debug our kernels from a wider perspective rather than at the individual thread-level.</li>
                    <li>&nbsp; &nbsp; &nbsp;- e.g. The H100(SXM5) can support up to ~270,000 threads at once. That's a lot of threads to manage <br>&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;without abstraction.</li>
                </ul>
                <h3>2. Easier Scheduling of Thread Executions</h3>
                <ul>
                    <li>The SIMT programming model needs a way for threads to coordinate operations. For example, if a group of threads writes to memory while another group reads from memory, we need a way to sync the operations.</li>
                    <li>Having the thread hierarchy allows scheduling threads easy.</li>
                </ul>
                <p>Note: One question above was "Aren't all thread executing the same instruction anyways?" Although the description of the SIMT model makes it seem like that is the case, it's actually more nuanced. In NVIDIA GPUs, instructions are executed in "warps", where each warp contains 32 threads. See <a href="#warps">aside on warps</a> section for more.</p>
                <h3>3. Needed to Use Shared Memory</h3>
                <ul>
                    <li>Thread hierarchy is also closely related with hardware.</li>
                    <li>Shared memory, an on-chip memory, is accessible at the thread block level. Each block is allocated a portion of the shared memory, which can be accessed by all threads within that block.</li>
                </ul>

        </ol>

        <p><strong>Note:</strong> Starting from the H100 series, there's now a new level added to the hierarchy called "cluster". So now the new CUDA thread hierarchy is <code>thread -> block -> cluster -> grid</code>.</p>

        <br>
        <h3 id="warps"><span style="font-weight: 1000;">Aside on Warps</span></h3>
        <p>Warps are a hardware constraint, which means the number of threads in a warp may change in future NVIDIA GPU architectures. Larger warp sizes will allow for more parallelism, but less flexibility whereas smaller warp sizes will have the opposite effect. 32 seems like the balance that NVIDIA has found and has stuck with for more than 10 years(GeForce 930M has warp size 32 as well).</p>

        <p>Now let's put these to practice and tackle the next couple puzzles.</p>

        <br>
        <h2>Puzzle 4: Map 2D</h2>
        <p><mark style="background: #d8ebffb4; font-weight:800">"Implement a kernel that adds 10 to each position of `a` and stores it in `out`. Input `a` is square. You have more threads than positions."</mark></p>

        <p>Two things to look out for: we are working with 2D matrixes now and we have more threads than positions.</p>

        <p>Working with 2D matrixes means that we should work with threads in the `x` dimension and the `y` dimension. Recall that threads can use up to `x`, `y`, and `z` dimensions.</p>

        <p>Also, CUDA follows the linear memory model, meaning that we can't do anything like <code>A[x][y]</code> but instead should do <code>A[x * width + y]</code> where <code>width</code> is the width of the matrix A.</p>

        <p>And having more threads than positions reminds us to set up guardrails as seen in Puzzle 3.</p>

        <p>Let's see how a kernel that combines all this looks like.</p>

        <pre><code class="language-cpp">constexpr int N = 100; // size of matrix A: [N,N]
// we have more threads than positions

__global__ void q4_solution_kernel(const float *A, float *out, int width) {
  int row = threadIdx.y;    //row uses y dim
  int col = threadIdx.x;    //col uses x dim 

  if (row < width && col < width) { //check bounds
    out[row * width + col] = A[row * width + col] + 10;
  }
}</code></pre>

        <p>That is it! 
        We're getting the hang of it now. Let's try something different to check if we got the hang of using 2D thread indexing.</p>

        <br>
        <h2>Puzzle 5: Broadcast</h2>
        <p><mark style="background: #d8ebffb4; font-weight:800">"Implement a kernel that adds `a` and `b` and stores it in `out`. Inputs `a` and `b` are vectors. You have more threads than positions."</mark></p>

        <p>To recap, our inputs are vectors while our output is a matrix. Let's draw out a diagram below to see what this looks like.</p>

        <img src="images/puzzle_5_vis.png" alt="Broadcasting vis." width="auto" height="300" style="margin-left:auto; margin-right: auto;">        

        <pre><code class="language-cpp">// We have more threads than positions
// a + b where a.shape=[N,1], b.shape=[1,N]
constexpr int N = 100;

__global__ void q5_solution_kernel(const float *a, const float *b, float *out, const int n) {
  int row = cuda.threadIdx.y;    //row uses y dim
  int col = cuda.threadIdx.x;    //col uses x dim

  if (row < n && col < n) {
    // assume a.shape = [N,1] (column vector)
    //        b.shape = [1,N] (row vector)
    out[row * n + col] = a[row] + b[col];
  }
}</code></pre>

        <p><strong>Note:</strong> It's important to know which axes you are working with. Drawing out the kernel you are implementing helps a lot.</p>

        <br>
        <h2>Using Multiple Thread Blocks</h2>

    <p>All the puzzles until now worked with a strong assumption: that our input data will be smaller than a single block. This assumption was there for pedagogical purposes, but does not hold in the real world. After all, GPUs excel doing repeated operations over large data, so we will have to deal with using multiple blocks.</p>

    <p>And this next puzzle is our first puzzle that shows this!</p>

    <br>
    <h2>Puzzle 6: Blocks</h2>
    <p><mark style="background: #d8ebffb4; font-weight:800">"Implement a kernel that adds 10 to each position of `a` and stores it in `out`. <em>You have fewer threads per block than the size of `a`.</em>"</mark></p>

    <p>Notice how we have fewer threads per block than the input vector. This indicates that we should use multiple blocks to launch enough threads to cover the vector.</p>

    <img src="images/multiple_blocks.png" alt="Using multiple blocks" width="auto" height="450" style="margin-left:auto; margin-right: auto;">        
    <br>

    <p>To tackle using multiple blocks, we need a way to calculate the global thread index. It is <br><code>int i = blockIdx.x * blockDim.x + threadIdx.x</code>. Let's see why.</p>

    <br>
    <img src="images/global_thread_index.png" alt="global thread indexing" width="auto" height="280" style="margin-left:auto; margin-right: auto;">        
    <br><br>

    <p>Let's apply this to solve our next puzzle. Let's take a look at the code below.</p>

    <pre><code class="language-cpp">constexpr int N = 1000;
constexpr int threads_per_block = 32;
// assume that an appropriate number of blocks is called for you

__global__ void q6_solution_kernel(const float *a, float *out, const int n) {
  int i = blockIdx.x * blockDim.x + threadIdx.x; // calculate global thread index

  if (i < n) {
    out[i] = a[i] + 10;
  }
}</code></pre>

    <br>
    <h3><span style="font-weight: 1000;">Aside on Deciding the Right Block Size</span></h3>
    <ul>
        <p>Choosing the right block size depends on which GPU you are using and the context of your kernel. A general tip is to stick to <code>32*n < 1024</code> for 1D vectors and <code>(32,32)</code> for 2D matrixes.</p>
        <p>This is because <code>1 warp = 32 threads</code> and <code>max threads per block = 1024</code> from V100s and onwards(until H100s, at the time of writing).</p>
        <p>Blocks that are too small will not take advantage of powerful parallelism ability of GPUs while blocks that are too large may drop thread utilization rates or stop the kernel from even running at all.</p>
        <p>&nbsp;&nbsp;&nbsp;<mark style="background: #fff2cf; font-weight:800">Example:</mark> Your input vector size is `101` but your block size is `100`. You will have to use 2 blocks where the second block will utilize only `1%` of your total threads.</p>
    </ul>

    <br>
    <p>Things are coming together now. We know how to deal with 2D data, having too many threads or too few threads. Let's test our knowledge by mixing up two previous puzzles into one: using multiple blocks on 2D matrixes.</p>

    <br>
    <h2>Puzzle 7: Blocks 2D</h2>
    <p><mark style="background: #d8ebffb4; font-weight:800">"Implement the same kernel in 2D. You have fewer threads per block than the size of `a` in both directions."</mark></p>

    <p>Remember that when working with 2D input data we have to write kernels that uses both `x` and `y` dimensions. This means we need to calculate the global thread indexes for both directions while taking into account that our blocks are also 2D.</p>

    <p>Visuals clarify things a lot when writing kernels. Let's see an example where <code>A.shape = [7,7]</code> and <br><code>BlockDim = (3,3,1)</code>.</p>

    <img src="images/2d_blocks.png" alt="2D blocking" width="auto" height="500" style="margin-left:auto; margin-right: auto;">        

    <p>Let's see what this would look like in code.</p>

    <pre><code class="language-cpp">constexpr int N = 10000;
constexpr int BLOCK_SIZE_X = 32;
constexpr int BLOCK_SIZE_Y = 32; // 2D block of size (32,32)
// assume that an appropriate number of blocks is called for you

__global__ void q7_solution_kernel(const float *A, float *out, const int width) {
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  int col = blockIdx.x * blockDim.x + threadIdx.x;

  if (row < width && col < width) {
    out[row * width + col] = A[row * width + col] + 10;
  }
}</code></pre>

    <p>That's it. There's only one key part remaining in the basic CUDA usage: using shared memory.</p>

    <br>
    <h2>Dive into Shared Memory</h2>

    <p>Loading data from DRAM is simple, but it's incredibly slow. Shared memory comes to the rescue by being an on-chip memory that is incredibly fast to read or write, with the caveat of being much smaller than DRAM. That means as CUDA programmers you have the power to pick which data can live on shared memory(...yes manually..for better or worse).</p>

    <p>First, how fast is "fast"? Let's jump ahead a bit and take a peek at how fast matmul becomes just by using shared memory(+tiling).</p>

    <p>I wrote a naive matmul vs shared memory mamtul with square matrixes of size `[4096,4096]` and block size `(16,16)` on my 2080ti. Here are the results:</p>

    <ul>
        <li>Naive matmul: <code>155.375 ms</code></li>
        <li>Shared memory matmul: <code>96.0123 ms</code></li>
    </ul>

    <p><mark style="background: #fff2cf;">Shared memory matmul is around 40% faster</mark>, with this effect to only expand with larger matrices. Although it's tempting to throw in shared memory everywhere after hearing this speedup, it shouldn't be used everywhere.</p>

    <h3><span style="font-weight: 1000;">Q. When Should We Use Shared Memory?</span></h3>
    <p>Here are the scenarios where using shared memory will make your kernel faster.</p>
    <ol>
            <h3><span style="font-weight: 800;">1. Data Reuse</span></h3>
            <ul>
                <li>If the same data is used multiple times, it would be efficient to cache them somewhere close instead of having to fetch them from global memory every time.</li>
                <li><mark style="background: #fff2cf;">e.g. Pooling, Convolution</mark></li>
            </ul>
          <h3><span style="font-weight: 800;">2. Threads Sharing Data</span></h3>
            <ul>
                <li>By loading data to shared memory, threads can access data that were fetched by other threads quickly.</li>
                <li><mark style="background: #fff2cf;">e.g. Parallel Reduction, such as Prefix Sum</mark></li>
            </ul>
          <h3><span style="font-weight: 800;">3. Accelerating Non-Coalesced Memory Accesses</span></h3>
            <ul>
                <li>Non-coalesced or strided memory accesses slow down kernels, but they are required for some applications(e.g. matrix transpose).</li>
                <li>By storing subsequent subsets("tile") of the data in shared memory, we can run non-coalesced memory accesses very quickly.</li>
                <li><mark style="background: #fff2cf;">e.g. Matrix Multiply</mark></li>
            </ul>
    </ol>

    <p>Now that we've seen how useful shared memory is, let's do a practice implementation with Puzzle 8.</p>

    <br>
    <h2>Puzzle 8: Shared</h2>
    <p><mark style="background: #d8ebffb4; font-weight:800">"Implement a kernel that adds 10 to each position of `a` and stores it in `out`. You have fewer threads per block than the size of `a`."</mark></p>

    <p>Spoiler: this puzzle is a bit odd since using shared memory on vector addition doesn't make it any faster.</p>
    <ul>
        <li>No data is reused, no need for threads to share data, and memory accesses are coalesced by default.</li>
    </ul>

    <p>We'll run through this puzzle to show how shared memory is implemented in CUDA and we'll look at where using shared memory is actually useful in Part 2 where we walk through optimizing parallel reduction in CUDA.</p>

    <p>Let's dive into the code.</p>

    <pre><code class="language-cpp">constexpr int N = 10000;
constexpr int BLOCK_SIZE = 256;

__global__ void q8_solution_kernel(const float *a, float *out, const int n) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;

  // init shared memory
  __shared__ float shared_a[BLOCK_SIZE];

  // copy elements into shared memory
  if (i < n) {
    shared_a[i] = a[i];
  }
  __syncthreads(); // sync threads to make sure memory write to shared is all finished before going further

  if (i < n) {
    // do vec add and write to out
    out[i] = shared_a[i] + 10.0f; // use shared_a instead of a
  }
}</code></pre>

<h3><span style="font-weight: 1000;">Q. What is <code>__syncthreads()</code>?</span></h3>
    <ul>
        <li>This is a barrier to make sure all threads finished instructions up to this point before moving further.</li>
        <li>Since we are dealing with parallel programs, we might have a situation where <code>thread 0</code> may have finished copying to <code>shared</code> but <code>thread 1</code> is still copying to <code>shared</code>.</li>
        <li>Without having barriers, subsequent code may run into undefined behavior by fetching elements in memory that are not written into yet.</li>
    </ul>

    <p>Make sure you always include thread barriers whenever you read or write data that will be used later in the code.</p>

    <br>
    <!-- <p>Now let's see a practical use of how shared memory makes a kernel faster.</p>

    <br>
    <h2>Puzzle 9: Pooling</h2>
    <p><mark style="background: #d8ebffb4; font-weight:800">"Implement a kernel that sums together the last 3 positions of `a` and stores it in `out`. You have 1 thread per position. You only need 1 global read and 1 global write per thread."</mark></p>

    <p>Implementing the general case of adding 3 positions is simple, but we have to consider what to do when our index is 0 or 1.</p>

    <p>For this particular problem, there's only 3 cases to worry about(when <code>i=0; i=1; i>=2</code>) and we can simply write them out.</p>

    <p>But wait, the problem said we have only "1 global read and 1 global write per thread." With our naive approach we are doing 3 global reads and 1 global write, because for each <code>i</code> we are reading from <code>a</code> 3 times<br>(<code>a[i] + a[i-1] + a[i-2]</code>).</p>

    <p>We can reduce the number of global reads by using shared memory: first we copy <code>a</code> into <code>shared</code> and then read from <code>shared</code> instead of <code>a</code>. This is an example of using shared memory for efficient Data Reusing--frequently accessed elements should be kept nearby instead of fetching from DRAM every time it's needed.</p>

    <p>Let's see the code below.</p>

    <pre><code class="language-cpp">constexpr int N = 10000;
constexpr int BLOCK_SIZE = 256;
...
__global__ void q9_solution_kernel(const float *a, float *out, const int n) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;

  // init shared memory
  __shared__ float shared_a[BLOCK_SIZE];

  // copy elements from a to shared
  if (i < n) {
    shared_a[i] = a[i];
  }
  __syncthreads();

  // compute pooling with edge cases i=0, i=1
  if (i == 0) {
    out[i] = shared[i];
  } elif (i == 1) {
    out[i] = shared[i] + shared[i - 1];
  } elif (i < n) {
    out[i] = shared[i] + shared[i - 1] + shared[i - 2];
  }
}</code></pre>
<br> -->

    <p>That finishes our introduction to CUDA programming! This was more on the basic functionalities available in CUDA and in Part 2 we'll get into the advanced use cases by optimizing a Parallel Reduction Sum kernel.</p>
    <p>I don't know how much CUDA is actively written in industry now, but learning it definitely led me to see the hardware side of software and the internal workings of a GPU.</p>
    <p>And the more I learn I see how it all goes back to Systems. If you're curious about these realizations of an undergrad going from ML methodology to Systems, continue reading more here(err still writing rn).</p>

    <br>
    <h2>References</h2>
    <p id="ref1">[1] <a href="https://github.com/srush/GPU-Puzzles" target="_blank">Sasha Rush, GPU Puzzles Github Repo</a></p>
    <p id="ref2">[2] <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html" target="_blank"> NVIDIA GPU Performance Guide</a></p>
    <p id="ref3">[3] <a href="https://developer.nvidia.com/cuda-gpus" target="_blank">NVIDIA CUDA Compute Capability Specs</a></p>
    <p id="ref4">[4] Programming Massively Parallel Processors(PMPP) Textbook, 4th ed. Hwu, Kirk, Hajj.</p>

    </main>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/line-numbers/prism-line-numbers.min.js"></script>
</body>
</html>