<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Thoughts on the Risks of AI</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <link rel="stylesheet" type="text/css" href="../../../style.css">
</head>
<body>
    <header>
        <h1>Thoughts on the Risks of AI</h1>
        <span class="date">July 1, 2024</span>
        <br><br>
        <a href="../../../index.html" class="about-me-link">Home</a>
        &nbsp; &nbsp;
        <a href="../../../about/about.html" class="about-me-link">About Me</a>
        &nbsp; &nbsp;
        <a href="../../non-technical.html" class="about-me-link">non-technical</a>
    </header>
    <main>
        <!-- <img src="images/seoul_forest_child.png" alt="child with mom" width="700" height="auto" style="margin-left:auto; margin-right: auto;"> -->
        <img src="images/seoul_forest.jpg" alt="child with mom" width="900" height="auto" style="margin-left:auto; margin-right: auto;">
        <br><br>
        <p>
            I don't think AI will take over the world in the ways we expect, such as becoming sentient and intently carrying out malicious acts. These are scenarios that we can very much imagine and organizations are readily setting up AI Safety teams to prevent such cases of misalignment. In other words, I'm not worried about these scenarios because <em>they are already on our radar</em> when we talk about "Risks of AI."
            <br><br>
            What worries me is how AI will change our abilities to think critically and increase addiction problems more than ever.
            <br><br>
            Here's a run-down of my top concerns:
        </p>
        <br>
        <h3>1. Decreased Attention Spans</h3>
        <p>
            The time to find an answer to a question has been greatly reduced by LLMs. This, of course, enables people to finish jobs faster than ever and it has greatly increased the "productivity" of my work as well. But when it comes to solving difficult problems, the time we take thinking deeply about how to solve the problem is as important as getting the answer right. And this process is taken away from us when we overuse LLMs in our lives.
            <br><br>
            For example, let's look at this question.
        </p>
        <p>
            <br>
            <span class="math-indent">\(\text{What is the last two digits of the following expression? }\)</span>
            <br><br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math-indent" style="margin-left: 16em;">\(\ 1! + 2! + \cdots + 18! + 19! + 20!\)</span>
            <br>
            <br>
            </p>
        <p>
            Although the first instinct is to compute all of them and then add them up, the key is to find a pattern to the series of consecutive factorials. 
        </p>
        <p><em>
            Note: For those curious, the last two digits from 10! and onwards all become zeros. This is because there's a trailing zero for every pair of (2,5) when the number is factorized and every number from 10! and above has two or more pairs of (2,5).
        </em>
        <br><br>
        Solving such problems trains us to not panic when we see difficult problems and instead to think critically about efficient approaches we could take. And, ironically, we come up with answers faster when we take the first couple minutes to just think about the problem without jumping into the equations or writing code right away.
        <br><br>
        This is indeed my grumpy grandpa moment who's waving his cane complaining about the world moving too fast, but I'm worried LLMs lower our bar on the expected time finding an answer to difficult problems.
        <br><br>
        <em>Note: </em>I'm sure people said similar stuff when we transitioned from encyclopedias to the internet. And if you ask me how this is any different, I honestly cannot give you a good answer why it feels different. Maybe it's because we can't judge the credibility of an LLM like how we judge credible sources on the internet? I'm still trying to figure this out.
    </p>

        <br>
        <h3>2. Addictive Generated Content</h3>
        <p>
            Recommendation systems in Instagram, Facebook, and YouTube perform extremely well recommending content to us we love, to a concerningly-addictive extent. But I'm worried about a future when generative models become so fast and powerful that they <em>generate</em> addictive content for us.
            <br><br>
            They could be images, videos, or even custom games or worlds we can spend time in. I could be talking to someone who I loved but passed away, my favorite game character, or so much more.
            <br><br>
            I know there are pros to this and it is an awesome technology for sure. The more privileged ones in our world will benefit from increased entertainment and therapeutic applications. But I'm worried that the less privileged ones of our world may suffer from mass addiction and seek such technologies as a place to spend most of their time in. My definition of "less privileged ones" are people who are more prone to addiction whether the contributing factors may be wealth, surrounding environment, or social relations.
            <br><br>
            In other words, <span class="bold-extra-bold"><em>powerful generative models may create a greater disparity between ones who can protect themselves against addiction and those who cannot.</em></span>
            <br><br>
            Sohl-Dickstein explained this concisely in his blogpost<a href="#ref1">[1]</a> when <em>"Future AI will be able to dynamically generate the text, audio, and video stimuli which is predicted to be most compelling to me personally, based upon the record of my past online interactions."</em>
        </p>

        <br>
        <p>
            Though I sound like an AI pessimist in this post, I'm actually quite the opposite. I'm excited to see how far AI will advance in my time along with how far science will advance with the help of AI. But to be prepared for such a world, we need to think of the un-thinkable harms that it may bring since they have the potential to rewire our brains in ways we might not want.
        </p>
        <br>
        <h2>References</h2>
    <p id="ref1">[1] <a href="https://sohl-dickstein.github.io/2023/09/10/diversity-ai-risk.html" target="_blank">"Brain dump on the diversity of AI risk" by Jascha Sohl-Dickstein.</a></p>
    </main>
</body>
</html>